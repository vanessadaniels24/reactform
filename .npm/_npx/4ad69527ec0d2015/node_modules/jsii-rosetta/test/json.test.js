"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
const node_buffer_1 = require("node:buffer");
const node_stream_1 = require("node:stream");
const json_1 = require("../lib/json");
describe(json_1.parse, () => {
    test('small value', async () => {
        const value = { foo: 'bar', baz: 123 };
        const jsonText = JSON.stringify(value);
        const readable = new node_stream_1.PassThrough();
        readable.end(jsonText);
        expect(await (0, json_1.parse)(readable)).toEqual(value);
    });
    test('value is too large to fit in a single string', async () => {
        // We'll leverage the fact JSON can contain multiple definitions of the same key multiple times...
        const expected = { foo: 'bar', baz: 123, bool: true, null: null, long: 'X'.repeat(102400) };
        const readable = node_stream_1.Readable.from((function* () {
            const chunks = Object.entries(expected).map(([key, value]) => `  ${JSON.stringify(key)}: ${JSON.stringify(value)}`);
            yield '{\n';
            let counter = 2;
            let emitComma = false;
            while (counter < node_buffer_1.kStringMaxLength) {
                for (const chunk of chunks) {
                    if (emitComma) {
                        yield ',\n';
                        counter += 2;
                    }
                    yield chunk;
                    counter += chunk.length;
                    emitComma = true;
                }
            }
            yield '\n}\n';
        })());
        const actual = await (0, json_1.parse)(readable);
        expect(actual).toEqual(expected);
    });
    test('invalid JSON input', () => {
        const readable = new node_stream_1.PassThrough();
        readable.end('{"bad": "JSON",');
        return expect((0, json_1.parse)(readable)).rejects.toThrowErrorMatchingInlineSnapshot(`"Parser cannot parse input: expected an object key"`);
    });
});
describe(json_1.stringify, () => {
    test('small value', async () => {
        const value = { foo: 'bar', baz: 123 };
        const jsonText = JSON.stringify(value);
        const chunks = new Array();
        const writable = new node_stream_1.Writable({
            write: (chunk, _encoding, callback) => {
                chunks.push(Buffer.from(chunk));
                callback(null);
            },
        });
        await (0, json_1.stringify)(value, writable);
        expect(Buffer.concat(chunks).toString('utf-8')).toBe(jsonText);
    });
    test('value too large for JSON text to fit in a string', async () => {
        const value = { key: 'X'.repeat(node_buffer_1.kStringMaxLength) };
        const chunks = new Array();
        const writable = new node_stream_1.Writable({
            write: (chunk, _encoding, callback) => {
                chunks.push(Buffer.from(chunk));
                callback(null);
            },
        });
        await (0, json_1.stringify)(value, writable);
        expect(headBytes(chunks, 10).toString('utf-8')).toBe('{"key":"XX');
        expect(tailBytes(chunks, 10).toString('utf-8')).toBe('XXXXXXXX"}');
    });
});
function headBytes(chunks, count) {
    if (chunks.length === 0) {
        return Buffer.alloc(0);
    }
    const [head, ...tail] = chunks;
    const headSlice = head.slice(0, count);
    if (headSlice.length === count) {
        return headSlice;
    }
    const tailSlice = headBytes(tail, count - headSlice.length);
    return Buffer.concat([headSlice, tailSlice]);
}
function tailBytes(chunks, count) {
    if (chunks.length === 0) {
        return Buffer.alloc(0);
    }
    const tail = chunks[chunks.length - 1];
    const tailSlice = tail.slice(Math.max(0, tail.length - count), tail.length);
    if (tailSlice.length === count) {
        return tailSlice;
    }
    const headSlice = tailBytes(chunks.slice(0, chunks.length - 1), count - tailSlice.length);
    return Buffer.concat([headSlice, tailSlice]);
}
//# sourceMappingURL=json.test.js.map